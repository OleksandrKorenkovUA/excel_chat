Stage 1: OpenWebUI enters the Pipe wrapper and initializes request context.
Input: The OpenWebUI connector calls the async method Pipe.pipe(body, __user__, __files__, __request__, __event_emitter__) in pipelines/pipe/pipe.py with a chat payload that can include user_message, messages, files or attachments, and streaming metadata. Processing: The wrapper resolves the active event emitter, decides whether to suppress internal progress messages with PIPE_SUPPRESS_PIPE_STATUS, and sets local emit helpers (emit_pipe and emit_marker_status) that can send status, message, and replace events back to OpenWebUI. Processing: Query text is extracted from raw user text and possible meta-task wrappers by _extract_user_query, _last_user_text, _message_text, and _normalize_query_text so that only the real user intent is forwarded. Output: A normalized query string and a list of candidate file references are prepared for downstream fetch and forwarding; if no usable query is found it returns early with None and does not call the pipelines service. Key code: pipelines/pipe/pipe.py functions Pipe.pipe, _extract_user_query, _last_user_text, _normalize_query_text, _emit, _emit_status.

Stage 2: The Pipe wrapper resolves uploaded files and converts them into transport payloads.
Input: The wrapper receives file references from body.files, body.attachments, message-level file parts, __files__, and body.file_id, where each object can carry id, file_id, file metadata, or content references. Processing: _extract_files deduplicates file objects by id, PICK_FIRST_FILE can limit to a single file, and for each selected file the wrapper calls OpenWebUI /api/v1/files/{fid}/content using httpx with headers from _pick_token or _auth_headers_from_request. Processing: Raw file bytes are held in memory, filename is optionally corrected from Content-Disposition via _name_from_cd, MIME type is inferred by _guess_mime, and bytes are base64-encoded into data_b64. Output: The wrapper builds file_payloads entries like {id, filename, content_type, data_b64} for the pipelines request, and also builds fallback_files tuples for fallback multipart upload if primary pipelines call fails. Key code: pipelines/pipe/pipe.py functions _extract_files, _pick_token, _auth_headers_from_request, _name_from_cd, _guess_mime, plus the GET call inside Pipe.pipe.

Stage 3: The Pipe wrapper sends an OpenAI-compatible request to pipelines and translates stream output for OpenWebUI.
Input: The wrapper creates payload = dict(body), then overwrites key fields with normalized query and files, explicitly sets payload["stream"] = True and payload["model"] = self.valves.PIPELINES_MODEL, and posts to {PIPELINES_BASE_URL}/chat/completions. Processing: For non-SSE responses it expects JSON like choices[0].message.content, while for SSE it parses data: lines, reading choices[0].delta content or choices[0].message content using _choice_delta_text and _choice_message_text. Processing: It strips embedded status markers in the format [[PIPELINE_STATUS:{...}]] via _extract_status_markers, forwards status text to event emitter, tracks progress token counts, and keeps only clean assistant text for final display. Output: On success it emits OpenWebUI replace events and returns final text; on exception it emits pipeline_error and can fallback to TARGET_API_URL with multipart form (query + files), returning fallback HTTP text if that call succeeds. Key code: pipelines/pipe/pipe.py functions Pipe.pipe, _extract_status_markers, _choice_delta_text, _choice_message_text, _emit_replace, _start_wait_ticker, _stop_wait_ticker.

Stage 4: The spreadsheet pipeline entrypoint selects sync or stream mode and extracts the effective user query.
Input: The pipelines service invokes Pipeline.pipe(user_message, model_id, messages, body, __event_emitter__) from pipelines/spreadsheet_analyst_pipeline.py with the request created in the wrapper stage. Processing: Pipeline.pipe routes to _pipe_sync when an emitter is available, or to _pipe_stream/_pipe_stream_inner when stream was requested without emitter, and both modes add request correlation IDs by _extract_request_trace_ids into contextvars for structured logs. Processing: _effective_user_query, _last_user_message, _is_meta_task_text, and _extract_user_query_from_meta strip system/meta instructions and search-query tasks, returning empty when the payload is a pure meta task without embedded user intent. Output: The stage yields either a normalized user question ready for file and analysis handling, or an early "skipped" final status when there is no real question to answer. Key code: pipelines/spreadsheet_analyst_pipeline.py functions Pipeline.pipe, _pipe_sync, _pipe_stream_inner, _extract_request_trace_ids, _effective_user_query, _is_search_query_meta_task.

Stage 5: Active file selection and session-aware reuse decide where file bytes come from.
Input: The pipeline receives body/messages plus optional session cache entry from _session_get(_session_key(body)), where session stores file_id, df_id, and prior profile fingerprint. Processing: _resolve_active_file_ref applies guard order (explicit current-turn file, then session file, then history fallback), preventing stale history file switches and exposing ignored_history_file_id for diagnostics. Processing: If the selected file matches an existing session df_id, the pipeline tries _sandbox_get_profile(df_id) and may reuse cached DataFrame state; otherwise it pulls metadata and bytes through _fetch_file_meta and _fetch_file_bytes, where _fetch_file_bytes supports inlined data_b64, file URL/content_url, filesystem path fallback, and finally OpenWebUI /api/v1/files/{file_id}/content. Output: The stage produces file_id plus either an already-loaded df_id/profile path or fresh raw bytes + metadata to load into sandbox; if file_id is missing it returns the user-visible message asking to attach CSV/XLSX. Key code: pipelines/spreadsheet_analyst_pipeline.py functions _resolve_active_file_ref, _pick_file_ref, _pick_file_ref_from_history, _session_get, _fetch_file_meta, _fetch_file_bytes, _sandbox_get_profile.

Stage 6: Sandbox load decodes bytes, parses the spreadsheet into a DataFrame, and builds normalized profile metadata.
Input: The pipeline sends _sandbox_load(file_id, meta, data) to sandbox_service /v1/dataframe/load with JSON fields file_id, filename, content_type, data_b64, max_rows, and preview_rows. Processing: sandbox_service.main.load_dataframe base64-decodes data_b64, selects parser in _load_dataframe by extension/content type (pd.read_excel for xlsx/xls, pd.read_parquet for parquet, pd.read_json(lines=True) for json/jsonl, pd.read_csv for csv/tsv), then truncates to max_rows if needed. Processing: Empty or malformed files fail fast in this layer because pandas read calls raise and the endpoint returns HTTP 400 detail like read_error:EmptyDataError:..., which the pipeline surfaces as sandbox_load_failed text. Processing: _profile_dataframe normalizes schema metadata into rows, cols, columns as strings, dtypes map, nulls_top, and preview records with object-cell truncation by _safe_trunc, and stores df + profile in in-memory DF_STORE with FILE_ID_INDEX mapping and TTL cleanup by _cleanup_store. Output: The pipeline gets {df_id, profile}, applies _apply_dynamic_limits using profile row count, writes session cache via _session_set, and now has a sandbox dataframe handle for execution; there is no temp-file persistence layer in this path because bytes and DataFrame stay in memory. Key code: pipelines/spreadsheet_analyst_pipeline.py _sandbox_load and _apply_dynamic_limits, sandbox_service/main.py endpoints load_dataframe and helpers _load_dataframe, _profile_dataframe, _cleanup_store.

Stage 7: Planner selection chooses retrieval shortcut, deterministic shortcut, or LLM code generation path.
Input: _prepare_analysis_code_for_question(question, profile, has_edit) receives normalized question text, dataframe profile schema/preview, and edit-intent signal from _has_edit_triggers. Processing: _select_router_or_shortcut first tries retrieval router when not edit, calling ShortcutRouter.shortcut_to_sandbox_code; router performs complexity and filter-context gating, optional LLM query normalization, QueryIR extraction, FAISS retrieval, candidate selection, slot filling, slot validation, compatibility checks, optional LLM verifier, and compile to pandas code. Processing: Missing or mismatched columns are handled here by design because _validate_slots emits invalid_column and unknown_column_ref issues, then the candidate is rejected and the router falls back to the next attempt or planner path instead of executing broken code. Processing: If router misses, the pipeline tries deterministic shortcut builders in sequence (_build_subset_keyword_metric_shortcut, _lookup_shortcut_code_from_slots via _llm_pick_lookup_slots, _ranking_shortcut_code via _llm_pick_ranking_slots, _template_shortcut_code, _edit_shortcut_code, _stats_shortcut_code), and also passes lookup_hints forward so the planner can preserve resolved filters. Output: The stage returns either router code + router_meta, shortcut code + short plan, or a signal to run full _plan_code LLM generation. Key code: pipelines/spreadsheet_analyst_pipeline.py functions _prepare_analysis_code_for_question and _select_router_or_shortcut, pipelines/shortcut_router/shortcut_router.py methods shortcut_to_sandbox_code, _retrieve_candidates, _pick_intent_from_candidates, _fill_slots, _validate_slots, _compile_plan.

Stage 8: LLM planning and code guardrails enforce schema-safe, subset-safe, and result-safe execution code.
Input: For full generation path, _plan_code gets question, df_profile, and optional lookup_hints, and sends a JSON payload to _llm_json with a system prompt from prompts.txt or DEFAULT_PLAN_CODE_SYSTEM enriched by _with_spreadsheet_skill_prompt. Processing: _llm_json calls OpenAI chat.completions.create with messages [{role:"system"},{role:"user"}] and temperature 0, expects a JSON object containing analysis_code, short_plan, op, and commit_df, and parses it with _parse_json_dict_from_llm which rejects non-object roots and raises on malformed payloads. Processing: This is the main schema-mismatch boundary for LLM responses, because fenced or noisy JSON is tolerated by _extract_json_candidate, but non-JSON or array-root outputs are treated as parse failure and push control into retry/exception paths instead of running arbitrary text. Processing: After initial plan, _prepare_analysis_code_for_question applies _enforce_count_code, _enforce_entity_nunique_code, and _finalize_code_for_sandbox; finalize validates subset filters, strips forbidden imports, blocks read-mode mutations, rewrites safe read rebinding, injects/removes COMMIT_DF, ensures result assignment, and can return explicit errors like missing_subset_filter, missing_result_assignment, or read mutation. Output: On certain validation failures it retries with stricter LLM constraints through _plan_code_retry_missing_filter, _plan_code_retry_missing_result, or _plan_code_retry_read_mutation, then resolves placeholders via _resolve_shortcut_placeholders and emits normalized code; otherwise it returns user-facing invalid_code/invalid_import/invalid_subset_filter-style failures. Key code: pipelines/spreadsheet_analyst_pipeline.py methods _plan_code, _plan_code_retry_missing_filter, _plan_code_retry_missing_result, _plan_code_retry_read_mutation, _llm_json, _parse_json_dict_from_llm, _finalize_code_for_sandbox.

Stage 9: Sandbox execution runs pandas code with AST/resource guards and runtime retry logic.
Input: The pipeline submits df_id + finalized code to /v1/dataframe/run via _sandbox_run, including timeout_s, preview_rows, max_cell_chars, and max_stdout_chars. Processing: sandbox_service.main.run_code invokes _run_code, where _ast_guard forbids imports, unsafe builtins, forbidden pandas I/O, and risky AST nodes, then executes code in a multiprocessing worker with CPU and memory limits, capturing stdout, result, COMMIT_DF, UNDO, and the post-exec df object. Processing: _run_code renders result_text via _render_result (DataFrame markdown/string preview, JSON for list/dict/array, scalar truncation), builds result_meta, computes mutation_summary and auto_commit flags by comparing df_before vs df_after, and returns status "ok", "err", or "timeout". Processing: Timeouts are explicit terminal responses from sandbox (status timeout, error \"Timeout after Ns\") and are not generally auto-retried by the pipeline except for the narrow KeyError import recovery branch. Output: Pipeline _run_analysis_with_retry returns run_resp and possibly retried code when a specific retryable runtime signature (KeyError on import) is detected; retry path uses _plan_code_retry_runtime_error or stats shortcut, re-finalizes code, and re-runs sandbox once. Key code: pipelines/spreadsheet_analyst_pipeline.py _run_analysis_with_retry and _sandbox_run, sandbox_service/main.py _ast_guard, _run_code, run_code.

Stage 10: Post-run commit/persistence logic decides whether edits were truly applied and what commit_failed means.
Input: _postprocess_run_result receives run_resp, analysis_code, edit_expected, prior profile fingerprint, and session references (session_key, file_id, df_id). Processing: If sandbox status is not ok, it returns structured failure with sandbox error text and mutation flags; otherwise it checks committed, auto_committed, structure_changed, and profile_changed signals and updates session profile when needed. Processing: In sandbox, persistence occurs only when undo_flag restores from history or when commit_flag/auto_commit_flag/structure_changed with a valid df_out triggers history push + entry["df"] replacement + profile recompute in run_code. Output: commit_failed is emitted exactly when edit_expected is true and analysis_code contains COMMIT_DF marker but none of committed, auto_committed, structure_changed, or profile_changed became true, meaning the edit path claimed a commit signal yet no observable dataframe state change was persisted. Key code: pipelines/spreadsheet_analyst_pipeline.py _postprocess_run_result, sandbox_service/main.py run_code history/commit branch, and commit marker checks around COMMIT_DF.

Stage 11: Final answer rendering and transport back to OpenWebUI chat window.
Input: The final rendering stage gets question, run status, result_text, result_meta, stdout, mutation_summary, mutation_flags, analysis code, and any runtime error fields. Processing: _final_answer first tries deterministic non-hallucinating formats for successful runs (_deterministic_answer, _aggregate_from_tabular_result, _format_table_from_result, _format_scalar_list_from_result, _format_availability_from_result), then uses edit-specific summaries when mutations are meaningful, and only falls back to LLM final wording when deterministic formatting is insufficient or execution is not clean. Processing: LLM final response uses DEFAULT_FINAL_ANSWER_SYSTEM with a JSON payload containing question, df_profile, plan, analysis_code, exec_status, stdout, result_text, result_meta, and error, then applies safety checks for unprompted code diagnostics and numeric hallucination with optional rewrite call using DEFAULT_FINAL_REWRITE_SYSTEM. Output: In stream mode _pipe_stream_inner yields status markers plus final text, in sync mode _pipe_sync returns final text directly, and the outer wrapper in pipelines/pipe/pipe.py converts pipelines /chat/completions JSON or SSE choices into clean assistant content, strips status markers, emits replace events, and returns the final string OpenWebUI shows in the chat bubble. Key code: pipelines/spreadsheet_analyst_pipeline.py functions _final_answer, _deterministic_answer, _pipe_sync, _pipe_stream_inner, _status_marker; pipelines/pipe/pipe.py streaming parser and return path around client.stream("POST", ... /chat/completions).
